{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up & downloading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torch in /home/coder/.local/lib/python3.7/site-packages (1.10.0)\nRequirement already satisfied: typing-extensions in /home/coder/.local/lib/python3.7/site-packages (from torch) (4.0.1)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: datasets in /home/coder/.local/lib/python3.7/site-packages (1.16.1)\nRequirement already satisfied: transformers in /home/coder/.local/lib/python3.7/site-packages (4.13.0)\nRequirement already satisfied: rouge-score in /home/coder/.local/lib/python3.7/site-packages (0.0.4)\nRequirement already satisfied: nltk in /home/coder/.local/lib/python3.7/site-packages (3.6.5)\nRequirement already satisfied: dill in /home/coder/.local/lib/python3.7/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: xxhash in /home/coder/.local/lib/python3.7/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (2021.11.1)\nRequirement already satisfied: packaging in /home/coder/.local/lib/python3.7/site-packages (from datasets) (20.4)\nRequirement already satisfied: numpy>=1.17 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (1.19.1)\nRequirement already satisfied: requests>=2.19.0 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (2.24.0)\nRequirement already satisfied: aiohttp in /home/coder/.local/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: pandas in /home/coder/.local/lib/python3.7/site-packages (from datasets) (1.1.2)\nRequirement already satisfied: tqdm>=4.62.1 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (4.62.3)\nRequirement already satisfied: multiprocess in /home/coder/.local/lib/python3.7/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/coder/.local/lib/python3.7/site-packages (from datasets) (0.2.1)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/coder/.local/lib/python3.7/site-packages (from datasets) (1.7.0)\nRequirement already satisfied: sacremoses in /home/coder/.local/lib/python3.7/site-packages (from transformers) (0.0.46)\nRequirement already satisfied: pyyaml>=5.1 in /home/coder/.local/lib/python3.7/site-packages (from transformers) (5.3.1)\nRequirement already satisfied: filelock in /home/coder/.local/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/coder/.local/lib/python3.7/site-packages (from transformers) (0.10.3)\nRequirement already satisfied: regex!=2019.12.17 in /home/coder/.local/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: six>=1.14.0 in /home/coder/.local/lib/python3.7/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: absl-py in /home/coder/.local/lib/python3.7/site-packages (from rouge-score) (1.0.0)\nRequirement already satisfied: joblib in /home/coder/.local/lib/python3.7/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: click in /home/coder/.local/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /home/coder/.local/lib/python3.7/site-packages (from packaging->datasets) (2.4.7)\nRequirement already satisfied: chardet<4,>=3.0.2 in /home/coder/.local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /home/coder/.local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2020.6.20)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/coder/.local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.25.10)\nRequirement already satisfied: idna<3,>=2.5 in /home/coder/.local/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.10)\nRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: asynctest==0.13.0; python_version < \"3.8\" in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: attrs>=17.3.0 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (20.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (2.0.9)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/coder/.local/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/coder/.local/lib/python3.7/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /home/coder/.local/lib/python3.7/site-packages (from pandas->datasets) (2020.1)\nRequirement already satisfied: zipp>=0.5 in /home/coder/.local/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.1.0)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: sentencepiece in /home/coder/.local/lib/python3.7/site-packages (0.1.96)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\nYou should consider upgrading via the '/usr/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
    }
   ],
   "source": [
    "! pip install torch\n",
    "! pip install datasets transformers rouge-score nltk\n",
    "! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'huggingface-cli login' in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\nsoftware-properties-common is already the newest version (0.96.20.2-2).\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n#!/bin/bash\n\nunknown_os ()\n{\n  echo \"Unfortunately, your operating system distribution and version are not supported by this script.\"\n  echo\n  echo \"You can override the OS detection by setting os= and dist= prior to running this script.\"\n  echo \"You can find a list of supported OSes and distributions on our website: https://packagecloud.io/docs#os_distro_version\"\n  echo\n  echo \"For example, to force Ubuntu Trusty: os=ubuntu dist=trusty ./script.sh\"\n  echo\n  echo \"Please email support@packagecloud.io and let us know if you run into any issues.\"\n  exit 1\n}\n\ngpg_check ()\n{\n  echo \"Checking for gpg...\"\n  if command -v gpg > /dev/null; then\n    echo \"Detected gpg...\"\n  else\n    echo \"Installing gnupg for GPG verification...\"\n    apt-get install -y gnupg\n    if [ \"$?\" -ne \"0\" ]; then\n      echo \"Unable to install GPG! Your base system has a problem; please check your default OS's package repositories because GPG should work.\"\n      echo \"Repository installation aborted.\"\n      exit 1\n    fi\n  fi\n}\n\ncurl_check ()\n{\n  echo \"Checking for curl...\"\n  if command -v curl > /dev/null; then\n    echo \"Detected curl...\"\n  else\n    echo \"Installing curl...\"\n    apt-get install -q -y curl\n    if [ \"$?\" -ne \"0\" ]; then\n      echo \"Unable to install curl! Your base system has a problem; please check your default OS's package repositories because curl should work.\"\n      echo \"Repository installation aborted.\"\n      exit 1\n    fi\n  fi\n}\n\ninstall_debian_keyring ()\n{\n  if [ \"${os}\" = \"debian\" ]; then\n    echo \"Installing debian-archive-keyring which is needed for installing \"\n    echo \"apt-transport-https on many Debian systems.\"\n    apt-get install -y debian-archive-keyring &> /dev/null\n  fi\n}\n\n\ndetect_os ()\n{\n  if [[ ( -z \"${os}\" ) && ( -z \"${dist}\" ) ]]; then\n    # some systems dont have lsb-release yet have the lsb_release binary and\n    # vice-versa\n    if [ -e /etc/lsb-release ]; then\n      . /etc/lsb-release\n\n      if [ \"${ID}\" = \"raspbian\" ]; then\n        os=${ID}\n        dist=`cut --delimiter='.' -f1 /etc/debian_version`\n      else\n        os=${DISTRIB_ID}\n        dist=${DISTRIB_CODENAME}\n\n        if [ -z \"$dist\" ]; then\n          dist=${DISTRIB_RELEASE}\n        fi\n      fi\n\n    elif [ `which lsb_release 2>/dev/null` ]; then\n      dist=`lsb_release -c | cut -f2`\n      os=`lsb_release -i | cut -f2 | awk '{ print tolower($1) }'`\n\n    elif [ -e /etc/debian_version ]; then\n      # some Debians have jessie/sid in their /etc/debian_version\n      # while others have '6.0.7'\n      os=`cat /etc/issue | head -1 | awk '{ print tolower($1) }'`\n      if grep -q '/' /etc/debian_version; then\n        dist=`cut --delimiter='/' -f1 /etc/debian_version`\n      else\n        dist=`cut --delimiter='.' -f1 /etc/debian_version`\n      fi\n\n    else\n      unknown_os\n    fi\n  fi\n\n  if [ -z \"$dist\" ]; then\n    unknown_os\n  fi\n\n  # remove whitespace from OS and dist name\n  os=\"${os// /}\"\n  dist=\"${dist// /}\"\n\n  echo \"Detected operating system as $os/$dist.\"\n}\n\nmain ()\n{\n  detect_os\n  curl_check\n  gpg_check\n\n  # Need to first run apt-get update so that apt-transport-https can be\n  # installed\n  echo -n \"Running apt-get update... \"\n  apt-get update &> /dev/null\n  echo \"done.\"\n\n  # Install the debian-archive-keyring package on debian systems so that\n  # apt-transport-https can be installed next\n  install_debian_keyring\n\n  echo -n \"Installing apt-transport-https... \"\n  apt-get install -y apt-transport-https &> /dev/null\n  echo \"done.\"\n\n\n  gpg_key_url=\"https://packagecloud.io/github/git-lfs/gpgkey\"\n  apt_config_url=\"https://packagecloud.io/install/repositories/github/git-lfs/config_file.list?os=${os}&dist=${dist}&source=script\"\n\n  apt_source_path=\"/etc/apt/sources.list.d/github_git-lfs.list\"\n  gpg_keyring_path=\"/usr/share/keyrings/github_git-lfs-archive-keyring.gpg\"\n\n  echo -n \"Installing $apt_source_path...\"\n\n  # create an apt config file for this repository\n  curl -sSf \"${apt_config_url}\" > $apt_source_path\n  curl_exit_code=$?\n\n  if [ \"$curl_exit_code\" = \"22\" ]; then\n    echo\n    echo\n    echo -n \"Unable to download repo config from: \"\n    echo \"${apt_config_url}\"\n    echo\n    echo \"This usually happens if your operating system is not supported by \"\n    echo \"packagecloud.io, or this script's OS detection failed.\"\n    echo\n    echo \"You can override the OS detection by setting os= and dist= prior to running this script.\"\n    echo \"You can find a list of supported OSes and distributions on our website: https://packagecloud.io/docs#os_distro_version\"\n    echo\n    echo \"For example, to force Ubuntu Trusty: os=ubuntu dist=trusty ./script.sh\"\n    echo\n    echo \"If you are running a supported OS, please email support@packagecloud.io and report this.\"\n    [ -e $apt_source_path ] && rm $apt_source_path\n    exit 1\n  elif [ \"$curl_exit_code\" = \"35\" -o \"$curl_exit_code\" = \"60\" ]; then\n    echo \"curl is unable to connect to packagecloud.io over TLS when running: \"\n    echo \"    curl ${apt_config_url}\"\n    echo \"This is usually due to one of two things:\"\n    echo\n    echo \" 1.) Missing CA root certificates (make sure the ca-certificates package is installed)\"\n    echo \" 2.) An old version of libssl. Try upgrading libssl on your system to a more recent version\"\n    echo\n    echo \"Contact support@packagecloud.io with information about your system for help.\"\n    [ -e $apt_source_path ] && rm $apt_source_path\n    exit 1\n  elif [ \"$curl_exit_code\" -gt \"0\" ]; then\n    echo\n    echo \"Unable to run: \"\n    echo \"    curl ${apt_config_url}\"\n    echo\n    echo \"Double check your curl installation and try again.\"\n    [ -e $apt_source_path ] && rm $apt_source_path\n    exit 1\n  else\n    echo \"done.\"\n  fi\n\n  echo -n \"Importing packagecloud gpg key... \"\n  # import the gpg key\n  curl -fsSL \"${gpg_key_url}\" | gpg --dearmor > ${gpg_keyring_path}\n  echo \"done.\"\n\n  echo -n \"Running apt-get update... \"\n  # update apt on this system\n  apt-get update &> /dev/null\n  echo \"done.\"\n\n  echo\n  echo \"The repository is setup! You can now install packages.\"\n}\n\nmain\n\n\n\n\ngit-lfs is already the newest version (2.7.1-1+deb10u1).\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nGit LFS initialized.\n"
    }
   ],
   "source": [
    "! sudo apt-get install software-properties-common\n",
    "! sudo curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh\n",
    "! sudo apt-get install git-lfs\n",
    "! git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/coder/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a summarization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/mt5-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'summary', 'idx'],\n        num_rows: 30000\n    })\n    validation: Dataset({\n        features: ['text', 'summary', 'idx'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['text', 'summary', 'idx'],\n        num_rows: 10000\n    })\n})"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "metric = datasets.load_metric(\"rouge\")\n",
    "\n",
    "#load through pandas and turn into Dataset format\n",
    "df = pd.read_csv(r'/work/danewsroom/danewsroom.csv', nrows = 50000)\n",
    "df = df.rename(columns={'Unnamed: 0': 'idx'})\n",
    "df_small = df[['text', 'summary', 'idx']]\n",
    "data = Dataset.from_pandas(df_small)\n",
    "\n",
    "#test train split\n",
    "train_d, test_d = data.train_test_split(test_size=0.2).values()\n",
    "#and validation\n",
    "train_d, val_d = train_d.train_test_split(test_size=0.25).values()\n",
    "\n",
    "#make the datasetdict\n",
    "dd = datasets.DatasetDict({\"train\":train_d,\"validation\":val_d,\"test\":test_d})\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"google/mtf5-small\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3c5471071d534db1a6e7efb752c23db7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65997640339d4078b3bd6a4b592ffde6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3fab56fed4141a082a7d67a667d2b3f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokenized_datasets = dd.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-summariser\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    #fp16=True,\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    metrics={k: round(v, 4) for k, v in result.items()}\n",
    "    np.save('mt5_metrics.npy', metrics) \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "The following columns in the training set  don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: idx, text, summary.\n***** Running training *****\n  Num examples = 30000\n  Num Epochs = 1\n  Instantaneous batch size per device = 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1875\n"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    }
   ],
   "source": [
    "#trainer.push_to_hub()\n",
    "#! git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd93434a9c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dd' is not defined"
     ]
    }
   ],
   "source": [
    "test_data = dd['test']\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "test_data = test_data.select(range(16))\n",
    "\n",
    "batch_size = 16  # change to 64 for full evaluation\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(\"cpu\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cpu\")\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"text\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"summary\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "np.save('mt5_results.npy', results)\n",
    "np.save('mt5_rouge.npy', rouge_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Summarization",
   "provenance": []
  },
  "interpreter": {
   "hash": "ad5c30fea5466ca3b04feb7e0a99857a92ee68be4c3f4107148abce45ea17a05"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit4d152f26c1db430c8196bba40bc19d2b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}