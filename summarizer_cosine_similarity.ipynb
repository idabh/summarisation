{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/coder/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "#this notebook is from github.com/AustinKrause/nyt-article-summarizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set columns to display all\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../Data/df_with_gensim_summaries.csv')\n",
    "df = pd.read_csv('../Summarization/danewsroom.csv', nrows = 1000)\n",
    "df = df.rename(columns={'Unnamed: 0': 'idx'})\n",
    "df = df[['text', 'summary', 'idx']]\n",
    "data = Dataset.from_pandas(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preview of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                text  \\\n0  En 20-årig mand fra Middelfart er blevet idømt...   \n1  Torsdag den 3. oktober 2013, 08:00\\n\\nI denne ...   \n2  - Lørdag den 15. august 2009, 05:26\\n\\nDer er ...   \n3  Den svenske psykolog Petra Krantz Lindgrens rå...   \n4  Berlingskes politiske kommentator ser det komm...   \n\n                                             summary  idx  \n0  Manden er bl.a. blevet dømt for tre tilfælde a...    1  \n1  Vi danskere er blevet mere og mere egoistiske ...    2  \n2  Der er indført undtagelsestilstand i bjergene ...    3  \n3  Den svenske psykolog Petra Krantz Lindgrens rå...    4  \n4  Berlingskes politiske kommentator ser det komm...    5  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>summary</th>\n      <th>idx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>En 20-årig mand fra Middelfart er blevet idømt...</td>\n      <td>Manden er bl.a. blevet dømt for tre tilfælde a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Torsdag den 3. oktober 2013, 08:00\\n\\nI denne ...</td>\n      <td>Vi danskere er blevet mere og mere egoistiske ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>- Lørdag den 15. august 2009, 05:26\\n\\nDer er ...</td>\n      <td>Der er indført undtagelsestilstand i bjergene ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Den svenske psykolog Petra Krantz Lindgrens rå...</td>\n      <td>Den svenske psykolog Petra Krantz Lindgrens rå...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Berlingskes politiske kommentator ser det komm...</td>\n      <td>Berlingskes politiske kommentator ser det komm...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding Cosine Similarity Between All Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"Drivers don’t always realize that they may be overpaying for car insurance. If you haven't compared quotes\n",
    "recently, even if you have a low rate, you could still be paying too much. Fortunately, millions of smart drivers have\n",
    "used EverQuote™'s free service to save hundreds on their insurance bills. It’s really no wonder that with so many \n",
    "drivers saving money, EverQuote™ is gaining momentum. EverQuote™ is an efficient source that tries to give consumers\n",
    "the lowest rates with tools you can trust. Just imagine what you could do with the money you save!\"\"\"\n",
    "\n",
    "sample = \"Det her er en dansk tekst.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2 = \"\"\"President Donald Trump and his Polish counterpart Andrzej Duda were to announce higher US troop levels in Poland \n",
    "on Wednesday, with the main question being whether Washington will defy Russian objections to establish an American \n",
    "base in the NATO country. A senior Trump administration official said the White House meeting would see the two \n",
    "leaders make a significant announcement.\" Whether Trump will risk irritating Moscow with a base or take the simpler \n",
    "option of adding more troops to the current non-permanent force was unclear. Located deep in what used to be \n",
    "Soviet-dominated eastern Europe, Poland is a member of NATO but has long wanted deeper US commitment. Spooked by \n",
    "resurgent Russia's seizing control of territory in Georgia and Ukraine over the last decade, Duda has tried to charm \n",
    "the US president, even touting the idea of Poland building a \"Fort Trump\" to house thousands of US soldiers.\n",
    "Krzysztof Szczerski, an adviser to the Polish president, said the general concept of a \"Fort Trump\" was on the \n",
    "agenda Wednesday and that the US presence \"will increase both in quality as well as quantity.\" \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will determine which sentences to extract from the article's text by finding the cosine similarity between all tf-idf transformed sentences. The extracted sentences will have the highest average cosine similarity to the remaining sentences. By doing this, the summary should include sentences that show the highest importance to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/82.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8053a2e1ea7d434b97407552e6a6bc7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/553 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce12e6a3572d4aa7b25c955d4372544b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/4.11M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6368ad2384b4dee95ee62ab8ccc0e04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/99.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a08d5296e40e4464960ebf860ddd0cf4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Det her er en test!', 'Hurra for det.']"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sentences = sent_tokenize(\"Det her er en test! Hurra for det.\")\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similarities(text):\n",
    "    #tokenize sentences\n",
    "    sentences = sent_tokenize(text, language = 'en')\n",
    "\n",
    "    #set stop words\n",
    "    stops = list(set(stopwords.words('english'))) + list(punctuation)\n",
    "    \n",
    "    #vectorize sentences and remove stop words\n",
    "    vectorizer = TfidfVectorizer(stop_words = stops)\n",
    "    #transform using TFIDF vectorizer\n",
    "    trsfm=vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    #creat df for input article\n",
    "    text_df = pd.DataFrame(trsfm.toarray(),columns=vectorizer.get_feature_names(),index=sentences)\n",
    "    \n",
    "    #declare how many sentences to use in summary\n",
    "    num_sentences = text_df.shape[0]\n",
    "    num_summary_sentences = int(np.ceil(num_sentences**.5))\n",
    "        \n",
    "    #find cosine similarity for all sentence pairs\n",
    "    similarities = cosine_similarity(trsfm, trsfm)\n",
    "    \n",
    "    #create list to hold avg cosine similarities for each sentence\n",
    "    avgs = []\n",
    "    for i in similarities:\n",
    "        avgs.append(i.mean())\n",
    "     \n",
    "    #find index values of the sentences to be used for summary\n",
    "    top_idx = np.argsort(avgs)[-num_summary_sentences:]\n",
    "    \n",
    "    return top_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sample text to determine the sentences to be extracted for the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/da.pickle\u001b[0m\n\n  Searched in:\n    - '/home/coder/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c810520cd930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-6830e07ad5bf>\u001b[0m in \u001b[0;36mfind_similarities\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#tokenize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'da'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m#set stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'danish'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/da.pickle\u001b[0m\n\n  Searched in:\n    - '/home/coder/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "find_similarities(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will call upon the find_similarities() function and will then arrange the sentences in the proper order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary(text):\n",
    "    #find sentences to extract for summary\n",
    "    sents_for_sum = find_similarities(text)\n",
    "    #sort the sentences\n",
    "    sort = sorted(sents_for_sum)\n",
    "    #display which sentences have been selected\n",
    "    print(sort)\n",
    "    \n",
    "    sent_list = sent_tokenize(text)\n",
    "    #print number of sentences in full article\n",
    "    print(len(sent_list))\n",
    "    \n",
    "    \n",
    "    #extract the selected sentences from the original text\n",
    "    sents = []\n",
    "    for i in sort:\n",
    "        sents.append(sent_list[i].replace('\\n', ''))\n",
    "    \n",
    "    #join sentences together for final output\n",
    "    summary = ' '.join(sents)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5]\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Fortunately, millions of smart drivers haveused EverQuote™'s free service to save hundreds on their insurance bills. It’s really no wonder that with so many drivers saving money, EverQuote™ is gaining momentum. Just imagine what you could do with the money you save!\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_summary(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A senior Trump administration official said the White House meeting would see the two leaders make a significant announcement.\" Whether Trump will risk irritating Moscow with a base or take the simpler option of adding more troops to the current non-permanent force was unclear. Located deep in what used to be Soviet-dominated eastern Europe, Poland is a member of NATO but has long wanted deeper US commitment.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_summary(sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 13, 15, 32, 46, 74, 76, 84]\n",
      "85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'      And never more so than in Showtime’s new series revival Some spoilers ahead through episode 4 of season 3 of Twin Peaks. On May 21st, Showtime brought back David Lynch’s groundbreaking TV series Twin Peaks, and fulfilled a prophecy in the process. In the second season finale, back in 1991, the spirit of series-defining murder victim Laura Palmer told FBI special agent and series protagonist Dale Cooper, “I’ll see you again in 25 years.” That clip plays again in the first episode of Lynch’s Twin Peaks revival, as a reminder that decades have in fact gone by, Laura’s promise has been carried out, and a series canceled mid-story is back on the air.A lot has changed in 25 years. And his development happened in parallel with the maturing of a TV audience that had to learn how to follow a new kind of story.All protagonists mediate their stories, but Dale Cooper is something else entirelyToday, viewers have more sophisticated expectations than they did in the 1990s. But Showtime’s new series is still leaving viewers curious, frustrated, baffled, and without the tools to translate what they’re seeing — which is exactly what Agent Cooper seems to be feeling right now as well. Like us, he didn’t really understand what was going on at that point of the story, but he was still optimistic, and still willing to roll with every strange new revelation. The series’s cancellation left viewers just as rudderless and abandoned as he was.‘Twin Peaks’ was about a hero maturing, but also about how the world consumed and broke himCooper’s trend toward impotence over the course of Twin Peaks’ original series run made for an ambitious and personal story. For now, he’s just along for the ride.we have so little basis for understanding our audience avatarThere’s no reason to believe that old Agent Cooper will ever return to Twin Peaks. In the same sort of way, no revival could ever recapture what Twin Peaks meant to audiences in 1990, when it was all so new and fresh and strange, so unlike anything that had ever appeared on network television.But he’s still around in some form, still bouncing through the trials of a Lynchian world. We’re still all Agent Cooper, navigating the mystery, and waiting to see where this is all going.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_summary(df.content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 15, 19, 20]\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Current systems at land borders in Hong Kong perform facial recognition through rolled-down windows, simply to avoid the confounding effect of the glass.But over the past few months, a new system has emerged to solve that problem, developed at Oak Ridge National Laboratory at the request of US Customs and Border Patrol. If effective, it could pave the way for far more aggressive deployment of facial recognition at automotive crossings.“The camera they have developed can go into a vehicle through tint and glare.”The system arose out of an initiative called biometric exit, which mandates a face or fingerprint verification of every US visitor as they exit the country. Colvin Pitts, a senior architect at Lytro, says the camera’s depth-sensing capability could be particularly useful when cleaning up an image for facial recognition. That’s consistent with Manaher’s early assessment, which described the system as “very prototypey.” “There is very little practical way to opt out of this system.”As the technology develops, it could enable ambitious new facial recognition projects, particularly when combined with automatic license plate recognition or ALPR technology. Last Year, New York governor Andrew Cuomo announced a plan called New York Crossings that would install facial recognition cameras alongside ALPR systems at every bridge and tunnel leading into Manhattan, although experts say there are still significant logistical challenges to such a system.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_summary(df.content[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}